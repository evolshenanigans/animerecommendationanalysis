{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af1010cf-ef62-4a8f-9679-93b59188e2e3",
   "metadata": {},
   "source": [
    "# day 1 set up & web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7be7526-00fc-4dab-81ef-6169aa753ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandasNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached pandas-2.2.3-cp39-cp39-win_amd64.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.22.4 (from pandas)\n",
      "  Using cached numpy-2.0.2-cp39-cp39-win_amd64.whl.metadata (59 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\gjgut\\miniconda3\\envs\\animeprojections\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gjgut\\miniconda3\\envs\\animeprojections\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached pandas-2.2.3-cp39-cp39-win_amd64.whl (11.6 MB)\n",
      "Using cached numpy-2.0.2-cp39-cp39-win_amd64.whl (15.9 MB)\n",
      "Using cached pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Using cached tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.0.2 pandas-2.2.3 pytz-2025.1 tzdata-2025.1\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "493cdeb9-35ab-4f59-8ef4-296b658b3171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1...\n",
      "Fetching page 2...\n",
      "Fetching page 3...\n",
      "Fetching page 4...\n",
      "Fetching page 5...\n",
      "Total anime fetched: 125\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Using Jikan API v4 (MyAnimeList unofficial API)\n",
    "base_url = \"https://api.jikan.moe/v4\"\n",
    "\n",
    "def get_anime_data(page=1, limit=25):\n",
    "    \"\"\"Get anime data from Jikan API with pagination\"\"\"\n",
    "    endpoint = f\"{base_url}/anime\"\n",
    "    params = {\n",
    "        'page': page,\n",
    "        'limit': limit,\n",
    "        'order_by': 'popularity',\n",
    "        'sort': 'asc'  # Most popular first\n",
    "    }\n",
    "    \n",
    "    response = requests.get(endpoint, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Fetch multiple pages (be mindful of rate limits)\n",
    "all_anime = []\n",
    "for page in range(1, 6):  # Fetching 5 pages (125 anime)\n",
    "    print(f\"Fetching page {page}...\")\n",
    "    data = get_anime_data(page=page)\n",
    "    \n",
    "    if data and 'data' in data:\n",
    "        all_anime.extend(data['data'])\n",
    "    else:\n",
    "        print(\"Failed to fetch data\")\n",
    "        \n",
    "    # Respect rate limiting (2 requests per second)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "print(f\"Total anime fetched: {len(all_anime)}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "anime_df = pd.DataFrame([{\n",
    "    'id': item['mal_id'],\n",
    "    'title': item['title'],\n",
    "    'english_title': item.get('title_english'),\n",
    "    'type': item['type'],\n",
    "    'episodes': item['episodes'],\n",
    "    'status': item['status'],\n",
    "    'airing': item['airing'],\n",
    "    'aired_from': item['aired']['from'],\n",
    "    'aired_to': item['aired']['to'],\n",
    "    'duration': item['duration'],\n",
    "    'rating': item['rating'],\n",
    "    'score': item['score'],\n",
    "    'scored_by': item['scored_by'],\n",
    "    'rank': item['rank'],\n",
    "    'popularity': item['popularity'],\n",
    "    'members': item['members'],\n",
    "    'favorites': item['favorites'],\n",
    "    'synopsis': item['synopsis'],\n",
    "    'season': item['season'],\n",
    "    'year': item['year'],\n",
    "    'studios': [studio['name'] for studio in item['studios']],\n",
    "    'genres': [genre['name'] for genre in item['genres']],\n",
    "    'themes': [theme['name'] for theme in item['themes']]\n",
    "} for item in all_anime])\n",
    "\n",
    "# Save to CSV\n",
    "anime_df.to_csv('anime_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28dbbc0f-a48a-4906-8636-29d81b1b1f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching reviews for anime ID: 16498\n",
      "Fetching reviews for anime ID: 1535\n",
      "Fetching reviews for anime ID: 5114\n",
      "Fetching reviews for anime ID: 30276\n",
      "Fetching reviews for anime ID: 38000\n",
      "Fetching reviews for anime ID: 11757\n",
      "Fetching reviews for anime ID: 31964\n",
      "Fetching reviews for anime ID: 11061\n",
      "Fetching reviews for anime ID: 20\n",
      "Fetching reviews for anime ID: 22319\n",
      "Fetching reviews for anime ID: 32281\n",
      "Fetching reviews for anime ID: 25777\n",
      "Fetching reviews for anime ID: 40748\n",
      "Fetching reviews for anime ID: 9253\n",
      "Fetching reviews for anime ID: 33486\n",
      "Fetching reviews for anime ID: 1735\n",
      "Fetching reviews for anime ID: 21\n",
      "Fetching reviews for anime ID: 35760\n",
      "Fetching reviews for anime ID: 19815\n",
      "Fetching reviews for anime ID: 28851\n"
     ]
    }
   ],
   "source": [
    "def get_anime_reviews(anime_id, page=1):\n",
    "    \"\"\"Get reviews for a specific anime\"\"\"\n",
    "    endpoint = f\"{base_url}/anime/{anime_id}/reviews\"\n",
    "    params = {'page': page}\n",
    "    \n",
    "    response = requests.get(endpoint, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Get reviews for the top 20 anime\n",
    "reviews_data = []\n",
    "for anime_id in anime_df['id'].head(20):\n",
    "    print(f\"Fetching reviews for anime ID: {anime_id}\")\n",
    "    reviews = get_anime_reviews(anime_id)\n",
    "    \n",
    "    if reviews and 'data' in reviews:\n",
    "        for review in reviews['data']:\n",
    "            reviews_data.append({\n",
    "                'anime_id': anime_id,\n",
    "                'user_username': review['user']['username'],\n",
    "                'score': review['score'],\n",
    "                'review': review['review']\n",
    "            })\n",
    "    \n",
    "    # Respect rate limiting\n",
    "    time.sleep(1)\n",
    "\n",
    "reviews_df = pd.DataFrame(reviews_data)\n",
    "reviews_df.to_csv('anime_reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4403a09a-37f8-45c3-90a3-2f1c655b62b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
